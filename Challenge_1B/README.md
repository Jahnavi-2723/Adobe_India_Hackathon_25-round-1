# ðŸ“„ Persona-Driven Document Intelligence

A lightweight, domain-agnostic system that extracts and summarizes the most relevant sections from a collection of PDFs based on a userâ€™s **persona** and their **job-to-be-done**.

---

## ðŸš€ Challenge Summary

**Theme:** Connect What Matters â€” For the User Who Matters  
**Objective:** Build an intelligent document analyzer that identifies and prioritizes sections from multiple PDFs based on:
- A specific **persona**
- A **task** the persona needs to accomplish  
- A **collection of PDFs** (from any domain)

---

## ðŸ“ Project Structure

```

.
â”œâ”€â”€ collection\_X/
â”‚   â”œâ”€â”€ input.json           # Persona, job, document list
â”‚   â”œâ”€â”€ pdf/                 # All input PDFs
â”‚   â””â”€â”€ output.json          # Final structured output
â”œâ”€â”€ requirements.txt         # Libraries needed
â”œâ”€â”€ main.py                  # Main processing script
â”œâ”€â”€ approach_explanation.md  # Methodology overview
â”œâ”€â”€ README.md                # Project summary and usage
â””â”€â”€ Dockerfile               # Containerized execution

````

---

## ðŸ› ï¸ How It Works

1. **Text Extraction:**  
   Uses `PyMuPDF` to extract text from every page of each PDF.

2. **Page Scoring with TF-IDF:**  
   Applies TF-IDF to determine which pages contain the most informative content.

3. **Top Page Selection:**  
   Picks top 5 pages based on TF-IDF scores using `heapq.nlargest`.

4. **Summarization:**  
   Uses a CPU-compatible **DistilBART** model to generate concise summaries.

5. **Output:**  
   Produces a structured `output.json` with:
   - Metadata
   - Top-ranked sections
   - Summarized content

---

## ðŸ§  Key Features

- âœ… Offline & CPU-Only (no internet required at runtime)
- âœ… Model stored in `model/` for size-optimized Docker image
- âœ… Efficient and interpretable output
- âœ… Domain-agnostic processing
- âœ… Containerized with Docker for portability

---

## ðŸ“¦ Requirements (If Running Locally)

Install dependencies using:

```bash
pip install -r requirements.txt
````

Key libraries used:

* `PyMuPDF`
* `torch`
* `transformers`
* `scikit-learn`

---

## â–¶ï¸ Running the Project (Locally)

1. Place all PDFs inside `collection_X/pdf/`
2. Create a corresponding `input.json` inside `collection_X/`
3. Run:

```bash
python main.py
```

4. Output will be saved to:

```bash
collection_X/output.json
```

---

## ðŸ³ Running with Docker (Recommended)

### ðŸ”§ Step 1: Build Docker Image

```bash
docker build -t adobe_hack_round1b:final .
```

### â–¶ï¸ Step 2: Run on a collection

Replace `collection_2` with your target folder:

```bash
docker run --rm \-v "%cd%/collection_2:/app/collection_2" \-e INPUT_DIR="/app/collection_2" \adobe_hack_round1b:final
```

> âœ… The script will automatically read from `INPUT_DIR/input.json` and write to `INPUT_DIR/output.json`.

---

## ðŸ“ Example Output

```json
{
  "metadata": {
    "input_documents": ["doc1.pdf", "doc2.pdf"],
    "persona": "HR professional",
    "job_to_be_done": "Create and manage fillable forms",
    "processing_timestamp": "2025-07-27T12:45:00"
  },
  "extracted_sections": [
    { "document": "doc1.pdf", "page_number": 2, "importance_rank": 1 }
  ],
  "subsection_analysis": [
    { "document": "doc1.pdf", "page_number": 2, "refined_text": "Summary text here..." }
  ]
}
```

---

## ðŸ§© Suggested Improvements

* Integrate Sentence-BERT for more semantic scoring
* Improve persona-task alignment using prompt engineering
* Add chunk-level summarization for dense pages

---

## ðŸ‘¤ Authors & Contributors

Made with â¤ï¸ for the **Adobe India Hackathon - Round 1B**

---

> "Extract what matters, for the one who matters."


